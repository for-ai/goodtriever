{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different subsets - minimal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../data/jigsaw/multilingual/\")\n",
    "files = list(base_dir.glob(\"*.json\"))\n",
    "\n",
    "langs = [\"en\", \"arb_Arab\", \"hin_Deva\", \"kor_Hang\", \"por_Latn\", \"rus_Cyrl\"]\n",
    "toxic_pattern = \"{lang}_toxicity_gte0.5_clean.json\"\n",
    "nontoxic_pattern = \"{lang}_toxicity_eq0_half_clean.json\"\n",
    "\n",
    "num_toxic, num_nontoxic = 3000, 10000\n",
    "\n",
    "base_toxic = pd.read_json(base_dir.parent / toxic_pattern.format(lang=\"\").strip(\"_\"))\n",
    "base_nontoxic = pd.read_json(\n",
    "    base_dir.parent / nontoxic_pattern.format(lang=\"\").strip(\"_\")\n",
    ")\n",
    "\n",
    "\n",
    "def get_sampled_indices(base_df, num_samples, num_sets, random_state):\n",
    "    return base_df.sample(\n",
    "        n=num_samples * num_sets, random_state=random_state\n",
    "    ).index.values.reshape((num_sets, num_samples))\n",
    "\n",
    "\n",
    "toxic_indices = get_sampled_indices(base_toxic, num_toxic, len(langs), random_state=42)\n",
    "nontoxic_indices = get_sampled_indices(\n",
    "    base_nontoxic, num_nontoxic, len(langs), random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = base_dir / \"different_subsets\"\n",
    "out_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# English\n",
    "toxic_df = base_toxic.loc[toxic_indices[0]]\n",
    "nontoxic_df = base_nontoxic.loc[nontoxic_indices[0]]\n",
    "\n",
    "toxic_out_file = out_folder / f\"en_toxicity_gte0.5_clean_{num_toxic}_sampled.json\"\n",
    "nontoxic_out_file = (\n",
    "    out_folder / f\"en_toxicity_eq0_half_clean_{num_nontoxic}_sampled.json\"\n",
    ")\n",
    "\n",
    "toxic_df.to_json(toxic_out_file, orient=\"records\")\n",
    "nontoxic_df.to_json(nontoxic_out_file, orient=\"records\")\n",
    "\n",
    "# Other languages\n",
    "for lang, toxic, nontoxic in zip(langs[1:], toxic_indices[1:], nontoxic_indices[1:]):\n",
    "    toxic_file = base_dir / toxic_pattern.format(lang=lang)\n",
    "    nontoxic_file = base_dir / nontoxic_pattern.format(lang=lang)\n",
    "\n",
    "    toxic_df = pd.read_json(toxic_file)\n",
    "    nontoxic_df = pd.read_json(nontoxic_file)\n",
    "\n",
    "    toxic_df = toxic_df.loc[toxic]\n",
    "    nontoxic_df = nontoxic_df.loc[nontoxic]\n",
    "\n",
    "    toxic_out_file = (\n",
    "        out_folder / f\"{lang}_toxicity_gte0.5_clean_{num_toxic}_sampled.json\"\n",
    "    )\n",
    "    nontoxic_out_file = (\n",
    "        out_folder / f\"{lang}_toxicity_eq0_half_clean_{num_nontoxic}_sampled.json\"\n",
    "    )\n",
    "\n",
    "    toxic_df.to_json(toxic_out_file, orient=\"records\")\n",
    "    nontoxic_df.to_json(nontoxic_out_file, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual datastore size - parallel data across languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../data/jigsaw/multilingual/\")\n",
    "files = list(base_dir.glob(\"*.json\"))\n",
    "\n",
    "langs = [\"en\", \"arb_Arab\", \"hin_Deva\", \"kor_Hang\", \"por_Latn\", \"rus_Cyrl\"]\n",
    "toxic_pattern = \"{lang}_toxicity_gte0.5_clean.json\"\n",
    "nontoxic_pattern = \"{lang}_toxicity_eq0_half_clean.json\"\n",
    "\n",
    "# 264K comments\n",
    "base_toxic = pd.read_json(base_dir.parent / toxic_pattern.format(lang=\"\").strip(\"_\"))\n",
    "# 1,1M comments\n",
    "base_nontoxic = pd.read_json(\n",
    "    base_dir.parent / nontoxic_pattern.format(lang=\"\").strip(\"_\")\n",
    ")\n",
    "\n",
    "\n",
    "def get_sampled_indices(base_df, num_samples, num_sets, random_state):\n",
    "    return base_df.sample(\n",
    "        n=num_samples * num_sets, random_state=random_state\n",
    "    ).index.values.reshape((num_sets, num_samples))\n",
    "\n",
    "\n",
    "num_toxic = [1000, 3000]  # [5000, 10000, 25000]\n",
    "num_nontoxic = [10000, 20000]  # [10000, 20000, 50000]\n",
    "\n",
    "toxic_indices = [\n",
    "    get_sampled_indices(base_toxic, num_samples, num_sets=1, random_state=42).reshape(\n",
    "        -1\n",
    "    )\n",
    "    for num_samples in num_toxic\n",
    "]\n",
    "nontoxic_indices = [\n",
    "    get_sampled_indices(\n",
    "        base_nontoxic, num_samples, num_sets=1, random_state=42\n",
    "    ).reshape(-1)\n",
    "    for num_samples in num_nontoxic\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: arb_Arab -- 1000 / 10000\n",
      "Language: hin_Deva -- 1000 / 10000\n",
      "Language: kor_Hang -- 1000 / 10000\n",
      "Language: por_Latn -- 1000 / 10000\n",
      "Language: rus_Cyrl -- 1000 / 10000\n",
      "Language: arb_Arab -- 3000 / 20000\n",
      "Language: hin_Deva -- 3000 / 20000\n",
      "Language: kor_Hang -- 3000 / 20000\n",
      "Language: por_Latn -- 3000 / 20000\n",
      "Language: rus_Cyrl -- 3000 / 20000\n"
     ]
    }
   ],
   "source": [
    "out_folder = base_dir / \"multilingual_size\"\n",
    "out_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for toxic_samples, nontoxic_samples in zip(toxic_indices, nontoxic_indices):\n",
    "    num_t = len(toxic_samples)\n",
    "    num_nt = len(nontoxic_samples)\n",
    "\n",
    "    # English\n",
    "    temp_toxic_df = base_toxic.loc[toxic_samples]\n",
    "    temp_nontoxic_df = base_nontoxic.loc[nontoxic_samples]\n",
    "\n",
    "    toxic_df = pd.concat([pd.DataFrame(), temp_toxic_df])\n",
    "    nontoxic_df = pd.concat([pd.DataFrame(), temp_nontoxic_df])\n",
    "\n",
    "    # Other languages\n",
    "    for lang in langs[1:]:\n",
    "        print(f\"Language: {lang} -- {num_t} / {num_nt}\")\n",
    "        toxic_file = base_dir / toxic_pattern.format(lang=lang)\n",
    "        nontoxic_file = base_dir / nontoxic_pattern.format(lang=lang)\n",
    "\n",
    "        temp_toxic_df = pd.read_json(toxic_file)\n",
    "        temp_nontoxic_df = pd.read_json(nontoxic_file)\n",
    "\n",
    "        temp_toxic_df = temp_toxic_df.loc[toxic_samples]\n",
    "        temp_nontoxic_df = temp_nontoxic_df.loc[nontoxic_samples]\n",
    "\n",
    "        toxic_df = pd.concat([toxic_df, temp_toxic_df])\n",
    "        nontoxic_df = pd.concat([nontoxic_df, temp_nontoxic_df])\n",
    "\n",
    "    toxic_out_file = out_folder / f\"multi_mid_toxicity_gte0.5_clean_{num_t}_each.json\"\n",
    "    nontoxic_out_file = (\n",
    "        out_folder / f\"multi_mid_toxicity_eq0_half_clean_{num_nt}_each.json\"\n",
    "    )\n",
    "\n",
    "    toxic_df.to_json(toxic_out_file, orient=\"records\")\n",
    "    nontoxic_df.to_json(nontoxic_out_file, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200000, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nontoxic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_safety",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
